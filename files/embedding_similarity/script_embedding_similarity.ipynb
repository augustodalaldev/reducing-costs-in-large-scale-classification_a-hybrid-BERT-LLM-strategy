{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c95af56",
   "metadata": {},
   "source": [
    "# LeanDL - 2025 - Testes\n",
    "\n",
    "Esse é um arquivo de script, focado em processar uma base de dados para o LeanDL 2025.\n",
    "\n",
    "Criado por Augusto Dalal (projeto OESNPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739e794",
   "metadata": {},
   "source": [
    "Instalação de Dependecias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pandas\n",
    "%pip install --upgrade pyarrow\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install sentence_transformers\n",
    "%pip install sklearn\n",
    "%pip install gc\n",
    "%pip install torch\n",
    "%pip install codecarbon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25377eb6",
   "metadata": {},
   "source": [
    "Módulo BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "class BERTSimilarity:\n",
    "    \"\"\"\n",
    "    Computes sentence-level similarity between user content and strategic themes using multilingual BERT embeddings.\n",
    "\n",
    "    Attributes:\n",
    "        df_person (pd.DataFrame): DataFrame containing user content.\n",
    "        df_strategic_themes (pd.DataFrame): DataFrame containing strategic themes and associated keywords.\n",
    "        id_col (str): Column name used as unique identifier in df_person.\n",
    "        uf_col (str): Column name representing the region/state.\n",
    "        summary_col (str): Column name in df_person containing the summary text.\n",
    "        theme_col (str): Column name in df_strategic_themes containing the theme.\n",
    "        keyword_col (str): Column name in df_strategic_themes containing keywords.\n",
    "        additional_text_cols (list[str]): Optional list of extra text columns in df_person to consider.\n",
    "        min_length (int): Minimum length of sentence to be considered.\n",
    "        model (SentenceTransformer): Pretrained multilingual BERT model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_person,\n",
    "        df_strategic_themes,\n",
    "        id_col: str = \"HASH_ID\",\n",
    "        uf_col: str = \"UF\",\n",
    "        summary_col: str = \"DS_RESUMO\",\n",
    "        theme_col: str = \"TEMA\",\n",
    "        keyword_col: str = \"PALAVRAS-CHAVE\",\n",
    "        additional_text_cols: list[str] = None,\n",
    "        embeddings_model: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "        min_length: int = 120,\n",
    "    ):\n",
    "        self.df_person = df_person\n",
    "        self.df_strategic_themes = df_strategic_themes\n",
    "        self.id_col = id_col\n",
    "        self.uf_col = uf_col\n",
    "        self.summary_col = summary_col\n",
    "        self.theme_col = theme_col\n",
    "        self.keyword_col = keyword_col\n",
    "        self.additional_text_cols = additional_text_cols or []\n",
    "        self.min_length = min_length\n",
    "        self.model = SentenceTransformer(embeddings_model)\n",
    "\n",
    "        self._validate_columns()\n",
    "\n",
    "    def _validate_columns(self):\n",
    "        \"\"\"\n",
    "        Validates if all required columns exist in the provided DataFrames.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any expected column is missing in df_person or df_strategic_themes.\n",
    "        \"\"\"\n",
    "        person_required = {self.id_col, self.summary_col}.union(set(self.additional_text_cols))\n",
    "        themes_required = {self.uf_col, self.theme_col, self.keyword_col}\n",
    "\n",
    "        missing_person = person_required - set(self.df_person.columns)\n",
    "        missing_themes = themes_required - set(self.df_strategic_themes.columns)\n",
    "\n",
    "        if missing_person:\n",
    "            raise ValueError(f\"Missing columns in df_person: {missing_person}\")\n",
    "        if missing_themes:\n",
    "            raise ValueError(f\"Missing columns in df_strategic_themes: {missing_themes}\")\n",
    "\n",
    "    def preprocess_metadata(self):\n",
    "        \"\"\"\n",
    "        Tokenizes and filters content from df_person, extracting sentences with sufficient length.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed DataFrame with columns [id_col, 'SOURCE', 'CONTENT'].\n",
    "        \"\"\"\n",
    "        output = []\n",
    "\n",
    "        for _, row in self.df_person.iterrows():\n",
    "            item_id = row.get(self.id_col)\n",
    "            summary = row.get(self.summary_col, '') or ''\n",
    "            if not isinstance(summary, str):\n",
    "                summary = ''\n",
    "\n",
    "            if summary.strip():\n",
    "                sentences = sent_tokenize(summary.strip(), language='portuguese')\n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    if len(sentence) >= self.min_length:\n",
    "                        output.append({\n",
    "                            self.id_col: item_id,\n",
    "                            'SOURCE': self.summary_col,\n",
    "                            'CONTENT': sentence\n",
    "                        })\n",
    "\n",
    "            for col in self.additional_text_cols:\n",
    "                value = row.get(col)\n",
    "                if isinstance(value, str) and value.strip():\n",
    "                    output.append({\n",
    "                        self.id_col: item_id,\n",
    "                        'SOURCE': col,\n",
    "                        'CONTENT': value.strip()\n",
    "                    })\n",
    "\n",
    "        self.df_person = pd.DataFrame(output)\n",
    "        return self.df_person\n",
    "\n",
    "    def compute_embeddings(self):\n",
    "        \"\"\"\n",
    "        Computes sentence embeddings for both themes and keywords using the multilingual model.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The df_strategic_themes DataFrame enriched with embedding columns.\n",
    "        \"\"\"\n",
    "        theme_embeddings = []\n",
    "        keyword_embeddings = []\n",
    "\n",
    "        for _, row in self.df_strategic_themes.iterrows():\n",
    "            theme = row.get(self.theme_col, '')\n",
    "            keywords = row.get(self.keyword_col, '')\n",
    "\n",
    "            theme_emb = self.model.encode(theme) if isinstance(theme, str) and theme.strip() else np.zeros(self.model.get_sentence_embedding_dimension())\n",
    "            theme_embeddings.append(theme_emb)\n",
    "\n",
    "            #alteração aqui\n",
    "            #keyword_embs = self.model.encode(keywords)\n",
    "            #avg_emb = np.mean(keyword_embs, axis=0)\n",
    "\n",
    "            #para isto (problema quando keywords é pequena)\n",
    "            if isinstance(keywords, list) and keywords:\n",
    "                keyword_embs = self.model.encode(keywords)\n",
    "                if keyword_embs.ndim == 1:\n",
    "                    keyword_embs = np.expand_dims(keyword_embs, axis=0)\n",
    "                avg_emb = np.mean(keyword_embs, axis=0)\n",
    "            else:\n",
    "                avg_emb = np.zeros(self.model.get_sentence_embedding_dimension())\n",
    "                \n",
    "\n",
    "            keyword_embeddings.append(avg_emb)\n",
    "\n",
    "        self.df_strategic_themes[f'{self.theme_col}_EMBEDDING'] = theme_embeddings\n",
    "        self.df_strategic_themes[f'{self.keyword_col}_EMBEDDING'] = keyword_embeddings\n",
    "\n",
    "        return self.df_strategic_themes\n",
    "\n",
    "    def match_content_to_strategic_themes(self):\n",
    "        \"\"\"\n",
    "        Computes cosine similarity between each sentence in df_person and each theme/keyword embedding.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with all matches and their similarity scores.\n",
    "        \"\"\"\n",
    "        result_rows = []\n",
    "\n",
    "        for _, person_row in self.df_person.iterrows():\n",
    "            item_id = person_row.get(self.id_col)\n",
    "            source = person_row.get('SOURCE')\n",
    "            content = person_row.get('CONTENT')\n",
    "            content_emb = self.model.encode(content)\n",
    "\n",
    "            for _, theme_row in self.df_strategic_themes.iterrows():\n",
    "                uf = theme_row.get(self.uf_col)\n",
    "                theme = theme_row.get(self.theme_col)\n",
    "                #keywords = theme_row.get(self.keyword_col)\n",
    "                theme_emb = theme_row.get(f'{self.theme_col}_EMBEDDING')\n",
    "                keyword_emb = theme_row.get(f'{self.keyword_col}_EMBEDDING')\n",
    "\n",
    "                theme_cos = float(cosine_similarity([content_emb], [theme_emb])[0][0])\n",
    "                keyword_cos = float(cosine_similarity([content_emb], [keyword_emb])[0][0])\n",
    "\n",
    "                result_rows.append({\n",
    "                    self.id_col: item_id,\n",
    "                    'SOURCE': source,\n",
    "                    'CONTENT': content,\n",
    "                    'UF': uf,\n",
    "                    self.theme_col: theme,\n",
    "                    f'{self.theme_col}_COSINE': theme_cos,\n",
    "                    #self.keyword_col: keywords,\n",
    "                    f'{self.keyword_col}_COSINE': keyword_cos\n",
    "                })\n",
    "\n",
    "        self.df_matches = pd.DataFrame(result_rows)\n",
    "        return self.df_matches\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Runs the full similarity pipeline: preprocess → embed → match.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Final DataFrame with matched content and similarity scores.\n",
    "        \"\"\"\n",
    "        self.preprocess_metadata()\n",
    "        self.compute_embeddings()\n",
    "        return self.match_content_to_strategic_themes()\n",
    "    \n",
    "    #adição, visando consumo de memória\n",
    "    def unload_model(self):\n",
    "        \"\"\"\n",
    "        Unloads the model from memory to free RAM.\n",
    "        \"\"\"\n",
    "        del self.model\n",
    "        self.model = None\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"BERT model successfully unloaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d7cab",
   "metadata": {},
   "source": [
    "Baixar Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gdown\n",
    "!gdown 12H957uf6mK-1X_ztT9hgFS1slpN2j-Wh\n",
    "!gdown 1-QXkqH8HzLcV2JCA4Nm9G5rQhorYKJVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10c25d",
   "metadata": {},
   "source": [
    "Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5391da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Inicio Tracking Carbono\n",
    "from codecarbon import EmissionsTracker\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "### Logging ---------------------------------------------------------------------------------\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import platform\n",
    "import psutil\n",
    "import os\n",
    "print(\"PID: \")\n",
    "print(os.getpid())\n",
    "\n",
    "# Configuração do log\n",
    "bert_logger = logging.getLogger(\"bert_logger\")\n",
    "bert_logger.setLevel(logging.INFO)\n",
    "\n",
    "# Handler para arquivo\n",
    "file_handler = logging.FileHandler(\"bert_similarity_LeanDL2025_log.txt\")\n",
    "file_handler.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\"))\n",
    "bert_logger.addHandler(file_handler)\n",
    "\n",
    "# Handler para console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "bert_logger.addHandler(console_handler)\n",
    "\n",
    "def log_memory(prefix=\"\"):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / (1024 ** 2)  # em MB\n",
    "    bert_logger.info(f\"{prefix} Memória usada: {mem:.2f} MB\")\n",
    "    return mem\n",
    "\n",
    "# Info do OS\n",
    "# Informações do processador\n",
    "cpu_count = psutil.cpu_count(logical=True)\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "cpu_name = platform.processor()\n",
    "\n",
    "bert_logger.info(f\"Processador: {cpu_name}\")\n",
    "bert_logger.info(f\"Núcleos lógicos: {cpu_count}\")\n",
    "if cpu_freq:\n",
    "    bert_logger.info(f\"Frequência máxima: {cpu_freq.max:.2f} MHz | Min: {cpu_freq.min:.2f} MHz | Atual: {cpu_freq.current:.2f} MHz\")\n",
    "    \n",
    "bert_logger.info(f\"Sistema operacional: {platform.system()} {platform.release()}\")\n",
    "bert_logger.info(f\"Versão do Python: {platform.python_version()}\")\n",
    "log_memory(\"Uso inicial\")\n",
    "\n",
    "# Timestamp de início\n",
    "start_time = datetime.now()\n",
    "bert_logger.info(f\"Início do processamento: {start_time}\")\n",
    "\n",
    "### ---------------------------------------------------------------------\n",
    "\n",
    "### Script --------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import  numpy as np\n",
    "#from filter_cv_bert.bert_similarity import BERTSimilarity\n",
    "import gc\n",
    "\n",
    "# Ajuste os caminhos se necessário\n",
    "path_dict = \"./leandl_oesnpg_dicionario.parquet\"\n",
    "path_data = \"./leandl_oesnpg_dados.parquet\"\n",
    "\n",
    "# Leitura usando pandas (requer pyarrow ou fastparquet)\n",
    "dicionario_df = pd.read_parquet(path_dict)\n",
    "dados_df = pd.read_parquet(path_data)\n",
    "\n",
    "#separa temas\n",
    "def to_fset(x):\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        return frozenset(x)\n",
    "    if pd.isna(x):\n",
    "        return frozenset()\n",
    "    return frozenset([x])\n",
    "\n",
    "df_temas = (\n",
    "    dados_df.assign(palavras_chave=dados_df[\"palavras_chave\"].apply(to_fset))\n",
    "            [[\"tema_id\", \"tema\", \"uf_tema_info\", \"palavras_chave\"]]\n",
    "            .drop_duplicates()\n",
    "            .assign(palavras_chave=lambda df: df[\"palavras_chave\"].apply(list))\n",
    ")\n",
    "\n",
    "df_perfis_id_tema = (\n",
    "    dados_df\n",
    "    .groupby(\"tema_id\")[\"hash_id\"]\n",
    "    .agg(lambda x: list(set(x)))\n",
    "    .reset_index()\n",
    "    .rename(columns={\"hash_id\": \"perfis_ids\"})\n",
    ")\n",
    "\n",
    "df_temas = df_temas.merge(df_perfis_id_tema, on=\"tema_id\")\n",
    "\n",
    "#separa perfis\n",
    "\n",
    "df_perfis = (\n",
    "    dados_df\n",
    "    .drop_duplicates(subset=[\"hash_id\"]).drop(columns=[\"tema_id\", \"tema\", \"palavras_chave\", \"uf_tema_info\", \"modelo_nivel\", \"modelo_explicacao\"])\n",
    ")\n",
    "\n",
    "# Deletar dataframes não utilizados\n",
    "del dicionario_df, dados_df\n",
    "\n",
    "# Iteração\n",
    "rows_temas = len(df_temas)\n",
    "\n",
    "results = []\n",
    "batch_size = 10\n",
    "\n",
    "k = 1\n",
    "for i, tema in df_temas.iterrows():\n",
    "\n",
    "    df_perfis_tema = df_perfis[df_perfis[\"hash_id\"].isin(tema[\"perfis_ids\"])].copy()\n",
    "\n",
    "    bert_logger.info(f\"Tema {k}/{rows_temas}: {tema['tema']} (id {tema['tema_id']})\")\n",
    "    log_memory(f\"Antes do processamento do tema {k}\")\n",
    "\n",
    "    bertAnalyzer = BERTSimilarity(df_person=df_perfis_tema,\n",
    "                                  df_strategic_themes=tema.to_frame().T,\n",
    "                                  id_col=\"hash_id\",\n",
    "                                  uf_col=\"uf_tema_info\",\n",
    "                                  summary_col=\"descricao_resumo\",\n",
    "                                  theme_col=\"tema\",\n",
    "                                  keyword_col=\"palavras_chave\",\n",
    "                                  additional_text_cols=sorted(set(df_perfis_tema.columns) - set([\"hash_id\", \"descricao_resumo\"]))\n",
    "                                  )\n",
    "    \n",
    "    df_resultado = bertAnalyzer.run_pipeline()\n",
    "    bertAnalyzer.unload_model()\n",
    "\n",
    "    df_resultado[\"tema_id\"] = tema[\"tema_id\"]\n",
    "\n",
    "    results.append(df_resultado)\n",
    "\n",
    "    log_memory(f\"Depois do processamento do tema {k}\")\n",
    "\n",
    "    if k % batch_size == 0:\n",
    "        #Salvar resultados (em partes)\n",
    "        df_batch = pd.concat(results, ignore_index=True)\n",
    "        df_batch.to_parquet(f\"./bert_similarity_results_batch_{k//batch_size}.parquet\", index=False)\n",
    "        results = []  # limpa lista de resultados da RAM\n",
    "        del df_batch, df_resultado, df_perfis_tema\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    k+=1\n",
    "\n",
    "if results:\n",
    "    df_batch = pd.concat(results, ignore_index=True)\n",
    "    df_batch.to_parquet(f\"./bert_similarity_results_batch_{k//batch_size + 1}.parquet\", index=False)\n",
    "    results = []\n",
    "    del df_batch\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# Fim Emissions\n",
    "emissions = tracker.stop()\n",
    "bert_logger.info(f\"Total de emissões (kg CO2): {emissions}\")\n",
    "\n",
    "# Timestamp de fim\n",
    "end_time = datetime.now()\n",
    "bert_logger.info(f\"Fim do processamento: {end_time}\")\n",
    "bert_logger.info(f\"Duração total: {end_time - start_time}\")\n",
    "log_memory(\"Uso final\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96459189",
   "metadata": {},
   "source": [
    "Concatenação Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "parquet_files = [f\"./bert_similarity_results_batch_{i}.parquet\" for i in range(1, 47)]\n",
    "df_final = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "df_final.to_parquet(\"./bert_similarity_results_LeanDL_2025.parquet\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeanDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
